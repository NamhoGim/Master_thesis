The recent advent of throughput computing with Graphics Processing Units (GPU) led an increase in demand for the memory bandwidth. To meet this increasing demand for memory bandwidth, 3D-stacked memory technologies such as High-Bandwidth Memory (HBM) \mycite{HBMstandard2015high} or Hybrid Memory Cube \mycite{hmc} have been proposed and now widely used in production GPUs and general-purpose processors. While these recent advances in memory system help throughput-oriented computing devices to exploit higher level of parallelism, this trend of increased memory bandwidth is also increasing the amount of power/energy spent on the memory system. For example, a future HBM-based memory system providing 4TB/s memory bandwidth for GPU is expected to use over 150W power~\mycite{subchannel17}. 

One of the primary components in DRAM energy consumption is the {\it row access energy} which is consumed when a DRAM row is activated (i.e., latch data of a DRAM row into the row buffer) and precharged (i.e., restore the bitline voltage). One main issue here is that the size of row is often much larger than the minimum amount of data that a GPU needs. For example, in HBM2 (with pseudo-channel mode \mycite{HBMstandard2015high}), the size of DRAM row is 1KB while the last level cacheline size of a GPU is often 128 bytes. This is not really a problem if there are many column accesses happening while the row is open. However, with a huge amount of parallelism provided by GPUs, a row is likely to close before several column accesses happen \mycite{subchannel17, connor2017finedram}. As a result, the amount of energy spent on row accesses remains a substantial component of DRAM energy consumption. 

To address this problem, partial row activation schemes~\mycite{copper2010fineact, udipi2010rethinking, connor2017finedram, subchannel17} have been proposed, which only activate part of the row that is likely to be accessed. By doing so, these proposals can avoid the problem of row over-fetching and reduce the amount of energy spent on row accesses. However, many of these proposals often incur significant area overhead reported to be 12\% to 34\%~\mycite{udipi2010rethinking,copper2010fineact}, which negatively affects both yield and capacity of DRAM. More importantly, most previous proposals require substantial changes to the standardized memory controller interface (e.g., JEDEC standard). While such changes may enable higher performance gains or energy savings, adopting such changes to a real system requires a significant amount of effort from multiple stakeholders, such as memory vendors, processor vendors, and memory standardization committee. Instead, we advocate more \emph{localized} solutions with minimal extensions to the existing memory interface for easy deployment.

Thus, we present a \emph{practical} partial row activation scheme which neither incurs noticeable area overhead nor requires any modification to the memory interface or controller. Exploiting the fact that many emerging GPU workloads, such as deep neural networks, are latency-tolerant by nature, we propose a partial row activation scheme requiring only minimal changes in DRAM and thus can be used as a drop-in replacement for existing, real HBM2 systems. The proposed ready-to-deploy scheme substantially reduces the amount of energy spent on row accesses at the expense of negligible performance degradation in most GPU workloads.

The rest of this thesis is organized as follows. Chapter 2 describes the features of deep learning workload and its DRAM access pattern on GPU and motivates this works. Chapter 3 elaborates on the proposed practical partial activation scheme which includes the bank structure for partial row activation and its timing. Chapter 4 presents the evaluation methodology and the results. Finally, we conclude the paper in Chapter 5.
