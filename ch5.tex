This thesis presents a practical partial activation scheme for 3D stacked DRAM with applications to deep learning workloads without significant modifications to the standard HBM2 DRAM interface. To achieve high energy efficiency while maintaining low performance overhead, we exploit the latency tolerance of emerging deep learning workloads to trade DRAM latency for energy efficiency. Consequently, our proposal demonstrates substantial DRAM energy savings with minimal performance hit for both the deep learning workloads and other conventional GPU workloads. This benefit comes with a very low implementation (area) cost and minimal adjustments of DRAM timing parameters over the standard HBM2 DRAM interface.
